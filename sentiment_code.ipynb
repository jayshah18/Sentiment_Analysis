{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayshah18/Sentiment_Analysis/blob/main/sentiment_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKgEQ8rweje3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiB0PuyKicRI",
        "outputId": "c199ca62-efca-4d4b-f53b-c4f671c9233a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as  np"
      ],
      "metadata": {
        "id": "WAs2ezFkfH76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "\n",
        "def review_dataset(dataset_path):\n",
        "    total_files = 0\n",
        "    summary = {}\n",
        "\n",
        "    for actor_folder in sorted(os.listdir(dataset_path)):\n",
        "        actor_path = os.path.join(dataset_path, actor_folder)\n",
        "        if os.path.isdir(actor_path):\n",
        "            file_list = [f for f in os.listdir(actor_path) if f.endswith(\".wav\") or f.endswith(\".mp3\")]\n",
        "            total_files += len(file_list)\n",
        "\n",
        "            durations = []\n",
        "            for file in file_list:\n",
        "                file_path = os.path.join(actor_path, file)\n",
        "                signal, sr = librosa.load(file_path, sr=None)\n",
        "                durations.append(librosa.get_duration(y=signal, sr=sr))\n",
        "\n",
        "            summary[actor_folder] = {\n",
        "                \"file_count\": len(file_list),\n",
        "                \"avg_duration_sec\": round(sum(durations)/len(durations), 2) if durations else 0\n",
        "            }\n",
        "\n",
        "    print(f\"\\n✅ Total Actors: {len(summary)}\")\n",
        "    print(f\"✅ Total Audio Files: {total_files}\\n\")\n",
        "\n",
        "    for actor, stats in summary.items():\n",
        "        print(f\"{actor}: {stats['file_count']} files | Avg Duration: {stats['avg_duration_sec']} sec\")\n",
        "\n",
        "# Run this\n",
        "review_dataset(\"/content/drive/MyDrive/DATASET\")\n"
      ],
      "metadata": {
        "id": "-9QexMixka7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a15bf54b-b34e-4a21-cf84-6ff3964292d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Total Actors: 24\n",
            "✅ Total Audio Files: 1440\n",
            "\n",
            "Actor_01: 60 files | Avg Duration: 3.75 sec\n",
            "Actor_02: 60 files | Avg Duration: 3.79 sec\n",
            "Actor_03: 60 files | Avg Duration: 3.76 sec\n",
            "Actor_04: 60 files | Avg Duration: 3.63 sec\n",
            "Actor_05: 60 files | Avg Duration: 3.74 sec\n",
            "Actor_06: 60 files | Avg Duration: 3.79 sec\n",
            "Actor_07: 60 files | Avg Duration: 3.75 sec\n",
            "Actor_08: 60 files | Avg Duration: 3.73 sec\n",
            "Actor_09: 60 files | Avg Duration: 3.49 sec\n",
            "Actor_10: 60 files | Avg Duration: 3.75 sec\n",
            "Actor_11: 60 files | Avg Duration: 3.44 sec\n",
            "Actor_12: 60 files | Avg Duration: 3.75 sec\n",
            "Actor_13: 60 files | Avg Duration: 3.33 sec\n",
            "Actor_14: 60 files | Avg Duration: 3.68 sec\n",
            "Actor_15: 60 files | Avg Duration: 3.5 sec\n",
            "Actor_16: 60 files | Avg Duration: 3.73 sec\n",
            "Actor_17: 60 files | Avg Duration: 3.67 sec\n",
            "Actor_18: 60 files | Avg Duration: 3.75 sec\n",
            "Actor_19: 60 files | Avg Duration: 3.87 sec\n",
            "Actor_20: 60 files | Avg Duration: 3.73 sec\n",
            "Actor_21: 60 files | Avg Duration: 3.92 sec\n",
            "Actor_22: 60 files | Avg Duration: 3.72 sec\n",
            "Actor_23: 60 files | Avg Duration: 3.61 sec\n",
            "Actor_24: 60 files | Avg Duration: 3.95 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def create_dataset_dataframe(dataset_path):\n",
        "    \"\"\"\n",
        "    Parses the RAVDESS dataset filenames to create a structured DataFrame.\n",
        "\n",
        "    The RAVDESS filename consists of a 7-part numerical identifier (e.g., 03-01-01-01-01-01-01.wav).\n",
        "    The 3rd part identifies the emotion.\n",
        "\n",
        "    Args:\n",
        "        dataset_path (str): The root path to the RAVDESS dataset (e.g., '/content/drive/MyDrive/DATASET').\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame with columns for filepath, actor, and emotion.\n",
        "    \"\"\"\n",
        "\n",
        "    # Emotion labels mapping from the RAVDESS documentation\n",
        "    emotion_map = {\n",
        "        \"01\": \"neutral\",\n",
        "        \"02\": \"calm\",\n",
        "        \"03\": \"happy\",\n",
        "        \"04\": \"sad\",\n",
        "        \"05\": \"angry\",\n",
        "        \"06\": \"fearful\",\n",
        "        \"07\": \"disgust\",\n",
        "        \"08\": \"surprised\"\n",
        "    }\n",
        "\n",
        "    data = []\n",
        "\n",
        "    print(f\"Parsing dataset from: {dataset_path}\")\n",
        "\n",
        "    for actor_folder in sorted(os.listdir(dataset_path)):\n",
        "        actor_path = os.path.join(dataset_path, actor_folder)\n",
        "        if os.path.isdir(actor_path):\n",
        "            for file_name in os.listdir(actor_path):\n",
        "                if file_name.endswith(\".wav\"):\n",
        "                    parts = file_name.split(\".\")[0].split(\"-\")\n",
        "\n",
        "                    # Ensure the filename has the correct number of parts\n",
        "                    if len(parts) == 7:\n",
        "                        emotion_code = parts[2]\n",
        "                        actor_id = parts[6]\n",
        "                        file_path = os.path.join(actor_path, file_name)\n",
        "\n",
        "                        # Get the emotion label from the map\n",
        "                        emotion_label = emotion_map.get(emotion_code)\n",
        "\n",
        "                        if emotion_label:\n",
        "                            data.append({\n",
        "                                \"filepath\": file_path,\n",
        "                                \"actor\": actor_id,\n",
        "                                \"emotion\": emotion_label\n",
        "                            })\n",
        "                        else:\n",
        "                            print(f\"Warning: Unknown emotion code '{emotion_code}' in file {file_name}\")\n",
        "\n",
        "    if not data:\n",
        "        print(\"\\nError: No data was loaded. Please check the following:\")\n",
        "        print(f\"1. Does the path '{dataset_path}' exist?\")\n",
        "        print(\"2. Is it the correct root folder containing the 'Actor_01', 'Actor_02', etc. subfolders?\")\n",
        "        return None\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    print(f\"\\n✅ Successfully created DataFrame with {len(df)} entries.\")\n",
        "    return df\n",
        "\n",
        "# --- HOW TO RUN THIS SCRIPT ---\n",
        "# 1. Make sure pandas is installed: pip install pandas\n",
        "# 2. Update the dataset_path to your actual path\n",
        "# 3. Run the script\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dataset_path = \"/content/drive/MyDrive/DATASET\"\n",
        "    ravdess_df = create_dataset_dataframe(dataset_path)\n",
        "\n",
        "    if ravdess_df is not None:\n",
        "        # Display the first few rows of the DataFrame\n",
        "        print(\"\\n--- DataFrame Head ---\")\n",
        "        print(ravdess_df.head())\n",
        "\n",
        "        # Display the distribution of emotions\n",
        "        print(\"\\n--- Emotion Distribution ---\")\n",
        "        print(ravdess_df['emotion'].value_counts())\n"
      ],
      "metadata": {
        "id": "6GurgsEEqFbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a6c47a-1182-45d0-9997-9235c246ca47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing dataset from: /content/drive/MyDrive/DATASET\n",
            "\n",
            "✅ Successfully created DataFrame with 1440 entries.\n",
            "\n",
            "--- DataFrame Head ---\n",
            "                                            filepath actor  emotion\n",
            "0  /content/drive/MyDrive/DATASET/Actor_01/03-01-...    01  neutral\n",
            "1  /content/drive/MyDrive/DATASET/Actor_01/03-01-...    01     calm\n",
            "2  /content/drive/MyDrive/DATASET/Actor_01/03-01-...    01  neutral\n",
            "3  /content/drive/MyDrive/DATASET/Actor_01/03-01-...    01     calm\n",
            "4  /content/drive/MyDrive/DATASET/Actor_01/03-01-...    01     calm\n",
            "\n",
            "--- Emotion Distribution ---\n",
            "emotion\n",
            "calm         193\n",
            "happy        192\n",
            "sad          192\n",
            "fearful      192\n",
            "angry        192\n",
            "disgust      192\n",
            "surprised    191\n",
            "neutral       96\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def create_dataset_dataframe(dataset_path):\n",
        "    \"\"\"\n",
        "    Parses the RAVDESS dataset filenames to create a structured DataFrame.\n",
        "    This is the same function from the previous script.\n",
        "    \"\"\"\n",
        "    emotion_map = {\n",
        "        \"01\": \"neutral\", \"02\": \"calm\", \"03\": \"happy\", \"04\": \"sad\",\n",
        "        \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\", \"08\": \"surprised\"\n",
        "    }\n",
        "    data = []\n",
        "    print(f\"Parsing dataset from: {dataset_path}\")\n",
        "    for actor_folder in sorted(os.listdir(dataset_path)):\n",
        "        actor_path = os.path.join(dataset_path, actor_folder)\n",
        "        if os.path.isdir(actor_path):\n",
        "            for file_name in os.listdir(actor_path):\n",
        "                if file_name.endswith(\".wav\"):\n",
        "                    parts = file_name.split(\".\")[0].split(\"-\")\n",
        "                    if len(parts) == 7:\n",
        "                        emotion_code = parts[2]\n",
        "                        actor_id = parts[6]\n",
        "                        file_path = os.path.join(actor_path, file_name)\n",
        "                        emotion_label = emotion_map.get(emotion_code)\n",
        "                        if emotion_label:\n",
        "                            data.append({\n",
        "                                \"filepath\": file_path,\n",
        "                                \"actor\": actor_id,\n",
        "                                \"emotion\": emotion_label\n",
        "                            })\n",
        "    if not data:\n",
        "        print(\"\\nError: No data was loaded.\")\n",
        "        return None\n",
        "    df = pd.DataFrame(data)\n",
        "    print(f\"\\n✅ Successfully created DataFrame with {len(df)} entries.\")\n",
        "    return df\n",
        "\n",
        "def extract_features(file_path):\n",
        "    \"\"\"\n",
        "    Extracts Mel Spectrogram and aggregated features (MFCC, Chroma, ZCR) from an audio file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load audio file at a consistent sample rate\n",
        "        signal, sample_rate = librosa.load(file_path, sr=22050)\n",
        "\n",
        "        # --- Features for Deep Learning Models (CNN, LSTM) ---\n",
        "        # Mel Spectrogram (for CNNs)\n",
        "        mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sample_rate, n_mels=128)\n",
        "\n",
        "        # --- Features for Traditional ML Models (SVM, Random Forest) ---\n",
        "        # MFCCs, Chroma, Zero-Crossing Rate\n",
        "        mfccs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=40)\n",
        "        chroma = librosa.feature.chroma_stft(y=signal, sr=sample_rate)\n",
        "        zcr = librosa.feature.zero_crossing_rate(y=signal)\n",
        "\n",
        "        # Aggregate features by taking the mean across time\n",
        "        mfccs_mean = np.mean(mfccs, axis=1)\n",
        "        chroma_mean = np.mean(chroma, axis=1)\n",
        "        zcr_mean = np.mean(zcr, axis=1)\n",
        "\n",
        "        # Combine aggregated features into a single feature vector\n",
        "        aggregated_features = np.hstack([mfccs_mean, chroma_mean, zcr_mean])\n",
        "\n",
        "        return mel_spectrogram, aggregated_features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# --- HOW TO RUN THIS SCRIPT ---\n",
        "# 1. Make sure pandas, librosa, numpy, and tqdm are installed.\n",
        "#    pip install pandas librosa numpy tqdm\n",
        "# 2. Update the dataset_path.\n",
        "# 3. Run the script. It will save a 'features_dataframe.pkl' file.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Initialize tqdm to work with pandas .apply()\n",
        "    tqdm.pandas()\n",
        "\n",
        "    dataset_path = \"/content/drive/MyDrive/DATASET\"\n",
        "    ravdess_df = create_dataset_dataframe(dataset_path)\n",
        "\n",
        "    if ravdess_df is not None:\n",
        "        print(\"\\nExtracting features from audio files... (This may take a few minutes)\")\n",
        "\n",
        "        # Apply the feature extraction function to each file\n",
        "        # The result of the lambda function is a tuple, which pandas expands into two new columns\n",
        "        ravdess_df[['mel_spectrogram', 'aggregated_features']] = ravdess_df['filepath'].progress_apply(\n",
        "            lambda filepath: pd.Series(extract_features(filepath))\n",
        "        )\n",
        "\n",
        "        # Drop rows where feature extraction might have failed\n",
        "        ravdess_df.dropna(inplace=True)\n",
        "\n",
        "        # Save the feature-rich dataframe to a pickle file for quick loading later\n",
        "        output_path = \"features_dataframe.pkl\"\n",
        "        ravdess_df.to_pickle(output_path)\n",
        "\n",
        "        print(f\"\\n✅ Feature extraction complete.\")\n",
        "        print(f\"✅ DataFrame saved to '{output_path}'\")\n",
        "        print(\"\\n--- DataFrame Head with New Features ---\")\n",
        "        print(ravdess_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1_Q2xgOpGyW",
        "outputId": "59646388-3e17-4895-cb43-0fa639781616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing dataset from: /content/drive/MyDrive/DATASET\n",
            "\n",
            "✅ Successfully created DataFrame with 1440 entries.\n",
            "\n",
            "Extracting features from audio files... (This may take a few minutes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1440/1440 [01:41<00:00, 14.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Feature extraction complete.\n",
            "✅ DataFrame saved to 'features_dataframe.pkl'\n",
            "\n",
            "--- DataFrame Head with New Features ---\n",
            "                                            filepath actor  emotion  \\\n",
            "0  /content/drive/MyDrive/DATASET/Actor_01/03-01-...    01  neutral   \n",
            "1  /content/drive/MyDrive/DATASET/Actor_01/03-01-...    01     calm   \n",
            "2  /content/drive/MyDrive/DATASET/Actor_01/03-01-...    01  neutral   \n",
            "3  /content/drive/MyDrive/DATASET/Actor_01/03-01-...    01     calm   \n",
            "4  /content/drive/MyDrive/DATASET/Actor_01/03-01-...    01     calm   \n",
            "\n",
            "                                     mel_spectrogram  \\\n",
            "0  [[1.9769283e-07, 3.2483592e-07, 1.3091303e-06,...   \n",
            "1  [[4.4146126e-10, 1.5210988e-08, 1.9769656e-08,...   \n",
            "2  [[1.3324016e-07, 1.4781753e-07, 1.0845676e-07,...   \n",
            "3  [[2.1944395e-09, 3.8604355e-08, 7.6826e-08, 1....   \n",
            "4  [[4.6863455e-08, 1.553473e-08, 6.2439507e-09, ...   \n",
            "\n",
            "                                 aggregated_features  \n",
            "0  [-691.587890625, 58.024662017822266, 0.1594650...  \n",
            "1  [-697.1661376953125, 65.10820007324219, 0.9303...  \n",
            "2  [-685.10546875, 55.87942123413086, 2.783262014...  \n",
            "3  [-707.3582153320312, 66.73645782470703, 2.2534...  \n",
            "4  [-734.1256103515625, 70.53291320800781, 4.2251...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8_twIIPhpmg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}